


# Import our dependencies
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.
import pandas as pd
application_df = pd.read_csv("https://static.bc-edx.com/data/dla-1-2/m21/lms/starter/charity_data.csv")
application_df.head()


# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.
application_df.drop(['EIN', 'NAME'], inplace=True, axis=1)


# Determine the number of unique values in each column.
application_df.nunique(dropna=True)


# Look at APPLICATION_TYPE value counts to identify and replace with "Other"
application_df['APPLICATION_TYPE'].value_counts()



# Choose a cutoff value and create a list of application types to be replaced
# use the variable name `application_types_to_replace`
application_types_to_replace = ['T17', 'T15', 'T29', 'T14', 'T25', 'T2', 'T12', 'T13', 'T9'] 
application_df['APPLICATION_TYPE'].replace(application_types_to_replace, 'Other', inplace=True)
# Replace in dataframe
for app in application_types_to_replace:
    application_df['APPLICATION_TYPE'] = application_df['APPLICATION_TYPE'].replace(app,"Other")

# Check to make sure replacement was successful
application_df['APPLICATION_TYPE'].value_counts()


# Look at CLASSIFICATION value counts to identify and replace with "Other"
application_df['CLASSIFICATION'].value_counts()



# You may find it helpful to look at CLASSIFICATION value counts >1
application_df['CLASSIFICATION'].value_counts()[application_df['CLASSIFICATION'].value_counts() > 1]



# Choose a cutoff value and create a list of classifications to be replaced
# use the variable name `classifications_to_replace`
classifications_to_replace = ['C1256', 'C1267', 'C1246', 'C1234', 'C3200', 'C0', 'C2710', 'C1260', 'C1600', 'C1257', 'C4100', 'C1720', 'C2400', 'C7210', 'C1237', 'C1235', 'C1278', 'C1238', 'C8200', 'C1250', 'C6000', 'C1800', 'C1500', 'C7120', 'C8000', 'C1240', 'C2300', 'C7200', 'C1400', 'C1230', 'C1280', 'C1300', 'C7100', 'C2800', 'C2700', 'C1270', 'C5000', 'C4000', 'C1700', 'C7000', 'C777', 'C1370', 'C1236', 'C2190', 'C4200', 'C5200', 'C6100', 'C2600', 'C1248', 'C1580', 'C1820', 'C1900', 'C2570', 'C1283', 'C3700', 'C2500', 'C1570', 'C2380', 'C1732', 'C1728', 'C2170', 'C4120', 'C8210', 'C4500', 'C1245', 'C2561', 'C2150'] 
application_df['CLASSIFICATION'].replace(classifications_to_replace, 'Other', inplace=True)

# Replace in dataframe
for cls in classifications_to_replace:
    application_df['CLASSIFICATION'] = application_df['CLASSIFICATION'].replace(cls,"Other")

# Check to make sure replacement was successful
application_df['CLASSIFICATION'].value_counts()


# Convert categorical data to numeric with `pd.get_dummies`
df_encoded = pd.get_dummies(application_df)
# Concatenate the encoded DataFrame with the original DataFrame
df_final = pd.concat([application_df, df_encoded], axis=1)


# Split our preprocessed data into our features and target arrays

# Assume df_final is your preprocessed DataFrame with features and target variable
X = df_final.drop("IS_SUCCESSFUL", axis=1)
y = df_final["IS_SUCCESSFUL"]

# Split the preprocessed data into a training and testing dataset
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify=y)

# Check the shapes of the arrays
print("X_train shape:", X_train.shape)
print("X_test shape:", X_test.shape)
print("y_train shape:", y_train.shape)
print("y_test shape:", y_test.shape)


# Create a StandardScaler instances
scaler = StandardScaler()

# Fit the StandardScaler
X_scaler = scaler.fit(X_train)

# Scale the data
X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)





# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.
#  YOUR CODE GOES HERE

nn = tf.keras.models.Sequential()

# First hidden layer
#  YOUR CODE GOES HERE

# Second hidden layer
#  YOUR CODE GOES HERE

# Output layer
#  YOUR CODE GOES HERE

# Check the structure of the model
nn.summary()


# Compile the model
#  YOUR CODE GOES HERE


# Train the model
#  YOUR CODE GOES HERE


# Evaluate the model using the test data
model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")


# Export our model to HDF5 file
#  YOUR CODE GOES HERE
